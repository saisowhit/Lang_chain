{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "hf_token=userdata.get('hftoken')\n",
    "from transformers import AutoTokenizer,AutoModelForCausalLM\n",
    "model_name=\"google/gemma-2b\"\n",
    "model=AutoModelForCausalLM.from_pretrained(model_name)\n",
    "tokenizer=AutoTokenizer.from_pretrained(model_name)\n",
    "print(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_text=\"What should I do if i face an issue\"\n",
    "input_ids=tokenizer(input_text,return_tensor=\"pt\")\n",
    "outputs=model.generate(**input_ids,max_length=120)\n",
    "print(tokenizer.decode(outputs[0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Based on Databricks dolly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Step2: Motivation for PEFT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "dataset_name=\"databricks/databricks-dolly-15k\"\n",
    "dataset=load_dataset(dataset_name,split=\"train[0:1000]\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def formatting_prompts(example):\n",
    "    output_texts=[]\n",
    "    for i in range(len(example['instruction'])):\n",
    "        if example['category'][i] in ['open_qa','general_qa']:\n",
    "            text=f\"Instruction:\\n{example['instruction']}\\n\\nResponse:\\n{example['response']}\"\n",
    "            output_texts.append(text)\n",
    "    return output_texts\n",
    "\n",
    "trainer=SFTTrainer(model,train_dataset=dataset,tokenizer=tokenizer,formatting_func=formatting_prompts)\n",
    "print(\"Intialized trainer for training\")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Finetuning using Parameter Effecient FineTuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create own dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "dataset_name=\"databricks/databricks-dolly-15k\"\n",
    "dataset=load_dataset(dataset_name,split=\"train\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "filtered_dataset=[]\n",
    "categories,count=defaultdict(int)\n",
    "for _,data in enumerate(dataset):\n",
    "    if data[\"context\"]:\n",
    "        continue\n",
    "    else:\n",
    "        text=f\"Instruction:\\n{data['instruction']}\\n\\nResponse:\\n{data['response']}\"\n",
    "        filtered_dataset.append(text)\n",
    "print(filtered_dataset[0:2])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jsonlines as jl\n",
    "with jl.open('dolly-mini-train.jsonl','w') as writers:\n",
    "    writer.write_all(filtered_dataset[0:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "dataset_name=\"ai-bites/databricks-mini\"\n",
    "dataset=load_dataset(dataset_name,split=\"train[0:100]\")\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name=\"google/gemma-2b\"\n",
    "new_model=\"gemma-ft\"\n",
    "lora_r=64\n",
    "lora_alpha=16\n",
    "lora_dropout=0.1\n",
    "use_4bit=True\n",
    "## Quantization type3\n",
    "## Two stges of quaninzation 1. Standard and Double Quantization\n",
    "bnb_4bit_quant_type=\"nf4\"\n",
    "output_dir='./results'\n",
    "num_train_epochs=1\n",
    "fp16=False\n",
    "bf16=False\n",
    "per_device_train_batch_size=2\n",
    "gradient_accumulation_steps=1\n",
    "gradient_checkpointing=True\n",
    "max_grad_norm=0.3\n",
    "learning_rate=\"2e-4\"\n",
    "weight_decay=0.001\n",
    "optim='paged_adam3_32bit'\n",
    "lr_scheduler_type=\"constant\"\n",
    "max_steps=-1\n",
    "warmup_ration=0.03\n",
    "group_by_length=True\n",
    "save_steps=25\n",
    "logging_steps=25\n",
    "max_seq_length=128\n",
    "packing=True\n",
    "device_map={\"\":0}\n",
    "device_map=\"auto\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load QLORA Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_dtype=getattr(torch,bnb_4bit_compute_dtype)\n",
    "bnb_config=BitsAndBytesConfig(load_in_4bit,bnb_4bit_quant_type=bnb_4bit_quant_type,bnb_4bit_compute_dtype=compute_dtype,bnb_4bit_use_double_quant=use_nested_quant,)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Checking GPU Comptability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if compute_dtype==torch.float16 and use_4bit:\n",
    "    major,_=torch.cuda.get_device_capability()\n",
    "    if major>=8:\n",
    "        print(\"Setting BF16 to True\")\n",
    "        bf16=True\n",
    "    else:\n",
    "        bs16=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load the model and Tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Loading basemodel\n",
    "model=AutoModelForCausalLM.from_pretrained(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_arguments=TrainingArguments(output_dir=output_dir,num_train_epochs=num_train_epochs,report_to=\"tensorboard\",learning_rate=learning_rate,max_grad_norm=max_grad_norm,max_steps=max_steps,warmup_ratio=warmup_ratio,fp16=fp16,group_by_length=group_by_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer=SFTTrainer(model=model,train_dataset=dataset,peft_config=peft_config,packing=packing,max_seq_length=max_seq_length,tokenizer=tokenizer,formatting_func=formatting_prompts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.model.save_pretrained(new_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Visualize Training on tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Incase of overfitting we can do two things Augugmentation, addiing more data to the training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir results/runs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Prompting the newly fine tuned model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_text=\"What should I do on a trip to Europe?\"\n",
    "base_model=AutoModelForCausalLM.from_pretrained(model_name,low_cpu_mem_usage=True,return_dict=True,torch_dtype=torch.float16,device_map=device_map)\n",
    "model=PeftModel.from_pretrained(base_model,new_model)\n",
    "model=model.merge()\n",
    "\n",
    "## Reload tokenizer to save it\n",
    "tokenizer=AutoTokenizer.from_pretrained(model_name,trust_remote_code=True)\n",
    "tokenizer.pad_token=tokenizer.eos_token\n",
    "tokenizer.padding_side=\"right\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## second Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import bitsandbytes as bnb\n",
    "from transformers import AuotTokenizer,AutoConfig,AutoModelForCasualLM\n",
    "model=AutoModelForCasualLM.from_pretrained(\"bigscience/bloom-7bl\",load_in_8bit=True,device_map='auto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer=AutoTokenizer.from_pretrained(\"bigscience/bloom-7bl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Freezing the weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in model.parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig,get_peft_model\n",
    "config=LoraConfig(r=16,lora_alpha=32,lora_dropout=0.06,bias=\"none\",task_type=\"CASUAL_LM\")\n",
    "model=get_peft_model(model,config)\n",
    "print_trainable_parameters(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Data\n",
    "import transformers\n",
    "from datasets import load_dataset\n",
    "data=load_dataset(\"Abirate/english_quotes\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_columns(example):\n",
    "    example[\"prediction\"]=example[\"quote\"]+\"->:\"+   str(example[\"tags\"])\n",
    "    return example\n",
    "\n",
    "data['train']=data['train'].map(merge_columns)\n",
    "data['train']['prediction'][:5]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=data.map(lambda samples:tokenizer(samples['prediction']),batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Training\n",
    "train=transformers.Trainer(model=model,train_dataset=data['train'],args=Transformers.TrainingArguments(per_device_train_batch_size=4,gradient_accumulation_steps=4,warmup_steps=100,max_steps=200,learning_rate=2e-4,fp16=True,logging_steps=1,output_dir='outputs'),data_collator=transformers.DataCollatroForLanguageModeling(tokenizer,mlm=False))\n",
    "model.config.use_cache=False\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## sharing the adapters on the hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.push_to_hub(\"samwit/bloom-7bl-lora-tagger\",use_auth_token=True,commit_message=\"basic training\",private=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load adapaters from Hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from peft import PeftModel,PeftConfig\n",
    "from transformers import AutoModelForCasualLM,AutoTokenizer\n",
    "peft_model_id=\"samwit/bloom-7b-lora-tagger\"\n",
    "config=PeftConfig.from_pretrained(peft_model_id)\n",
    "model=AutoModelForCausalLM.from_pretrained(config.base_model_name_or_path,return_dict=True,load_in_8bit=True)\n",
    "Tokenizer=AutoTokenizer.from_pretrained(config.base_model_name_or_path)\n",
    "model=PeftModel.from_pretrained(model,peft_model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch=  tokenizer(\"the world is your carrer ->\",return_tensors=\"pt\")\n",
    "with torch.cuda.amp.autocast():\n",
    "    output_tokens=model.generate(**batch,max_new_tokens=50)\n",
    "print('\\n \\n',tokenizer.decode(output_tokens[0],skip_special_token=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.youtube.com/watch?v=Us5ZFp16PaU&t=518s"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
