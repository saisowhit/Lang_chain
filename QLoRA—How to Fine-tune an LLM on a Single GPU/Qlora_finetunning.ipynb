{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCasualLM,AutoTokenizer,pipeline\n",
    "from peft import prepare_model_for_kbit_training\n",
    "from peft import LoraConfig,get_peft_model\n",
    "from datasets import load_dataset\n",
    "import transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name=\"TheBloke/Mistral-7b-Instruct-v0.2-GPTQ\"\n",
    "model=AutoModelForCasualLM.from_pretrained(model_name,device_map=\"auto\",trust_remote_code=False,revision=\"main\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer=AutoTokenizer.from_pretrained(model_name,use_Fast=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "comment=\"Great Content, thank you\"\n",
    "prompt=f'''[INST] {comment} [/INST]'''\n",
    "## tokenize input\n",
    "inputs=tokenizer(prompt,return_tensors=True)\n",
    "outputs=model.generate(input_ids=inputs[\"input_ids\"].to(\"cuda\"))\n",
    "print(tokenizer.batch_decode(outputs)[0])"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
